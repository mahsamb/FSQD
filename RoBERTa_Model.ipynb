{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Exploration of FQSC model"
      ],
      "metadata": {
        "id": "z6-5uw0i5vvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Notebook\n",
        "\n",
        "This notebook commences with the loading of the FQSD. Following the dataset preparation, we delve into the application of the RoBERTa model, a state-of-the-art natural language processing tool, to analyze and extract insights from the FQSD. Our goal is to leverage RoBERTa's advanced capabilities to understand the nuances within the dataset, enabling us to classify and interpret the questions more effectively.\n"
      ],
      "metadata": {
        "id": "OjWrQXN45vwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Download and Processing**:\n",
        "\n",
        "This code snippet demonstrates the process of downloading a dataset from a remote URL, unzipping it, and loading it into a Pandas DataFrame. The dataset, in this case, is the FSQD-Json-dataset. This initial step is crucial for preparing the data for subsequent analysis and visualization."
      ],
      "metadata": {
        "id": "EUfPkud55vwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data_url = \"https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip\"\n",
        "zip_filename = \"FSQD-Json-dataset.zip\"\n",
        "\n",
        "# Downloading using requests\n",
        "response = requests.get(data_url)\n",
        "\n",
        "# Check if the request was successful (status_code 200)\n",
        "if response.status_code == 200:\n",
        "    with open(zip_filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the data: {response.status_code}: {response.text}\")\n",
        "    # Add additional error handling here\n",
        "\n",
        "# Unzipping the dataset\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"FSQD-Json-dataset\")\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir(\"FSQD-Json-dataset\"))\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The file doesn’t appear to be a valid zip file\")\n",
        "\n",
        "\n",
        "\n",
        "json_file_path = 'FSQD-Json-dataset/FSQD-Json-dataset.json'  # Update with the correct file path\n",
        "\n",
        "# Try reading the file as a JSON Lines file\n",
        "try:\n",
        "    merged_df = pd.read_json(json_file_path, lines=True)\n",
        "except ValueError as e:\n",
        "    print(f\"Error reading the JSON file: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T08:30:24.660040Z",
          "iopub.execute_input": "2024-02-25T08:30:24.660644Z",
          "iopub.status.idle": "2024-02-25T08:30:27.892160Z",
          "shell.execute_reply.started": "2024-02-25T08:30:24.660608Z",
          "shell.execute_reply": "2024-02-25T08:30:27.890800Z"
        },
        "trusted": true,
        "id": "wAAzwFCy5vwN",
        "outputId": "5167ebcb-fe0b-4a7e-8c6c-7c875e5693fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files extracted:\n['ConvEx-Json-dataset.json', 'Yu_et_al_2012-Json-dataset.json', 'SubjQA-Json-dataset.json', 'FSQD-Json-dataset.json']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T08:30:27.894463Z",
          "iopub.execute_input": "2024-02-25T08:30:27.894907Z",
          "iopub.status.idle": "2024-02-25T08:30:27.900988Z",
          "shell.execute_reply.started": "2024-02-25T08:30:27.894867Z",
          "shell.execute_reply": "2024-02-25T08:30:27.899918Z"
        },
        "trusted": true,
        "id": "JlOVWQf45vwT",
        "outputId": "f2389d73-466b-4987-be70-1b3286f87954"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Index(['Question', 'Label_FSQD', 'Label_Subjectivity', 'Label_ComparisionForm',\n       'Label_Subjectivity_ComparisionForm', 'Label_SubjectivityType'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads and processes the data from [FSQD-Json-dataset](https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip)."
      ],
      "metadata": {
        "id": "iwtFgfKD5vwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "3efmxcms5vwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup for Deep Learning\n",
        "\n",
        "Before diving into the model training and evaluation, we need to set up our environment with all the necessary libraries and frameworks:\n"
      ],
      "metadata": {
        "id": "VvroeBis5vwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow sklearn transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T08:30:27.907939Z",
          "iopub.execute_input": "2024-02-25T08:30:27.908348Z",
          "iopub.status.idle": "2024-02-25T08:30:31.077069Z",
          "shell.execute_reply.started": "2024-02-25T08:30:27.908310Z",
          "shell.execute_reply": "2024-02-25T08:30:31.076014Z"
        },
        "trusted": true,
        "id": "ZAeUEuvZ5vwU",
        "outputId": "9abf19d2-3109-4919-d674-58d4261252b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "uIBITAvi5vwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation with RoBERTa\n",
        "\n",
        "This section of the code details our machine learning pipeline, where we focus on training and evaluating a RoBERTa-based model for sequence classification tasks within the FQSD. We initiate by tokenizing the dataset questions using RoBERTa's tokenizer and encoding the labels into integers for processing. Our model, built upon the TFRobertaForSequenceClassification architecture, includes a dropout layer for regularization and a dense layer tailored to our dataset's number of classes.\n",
        "\n",
        "We leverage K-Fold cross-validation, specifically a 5-fold strategy, to ensure our model's robustness and generalizability. For each fold, we train the model, track its history, and evaluate its performance using precision, recall, and F1-score metrics. This approach not only validates our model's effectiveness but also provides a comprehensive overview of its predictive capabilities.\n",
        "\n",
        "By iterating through each fold, we collect a set of metrics that, once macro-averaged, offer insights into the overall performance of our model across different subsets of the data. The precision, recall, and F1-scores for each fold are reported, followed by the macro-averaged values, culminating in a robust assessment of our classification model's performance.\n"
      ],
      "metadata": {
        "id": "LothAXsr5vwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "f5Ykldbm5vwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Run for Model Evaluation\n",
        "\n",
        "In alignment with our article's methodology, the final results presented are derived from the average of five separate runs to ensure the robustness and reliability of our findings. The code segment showcased here represents just one of these runs, offering a glimpse into the process of training and evaluating our RoBERTa-based model on the FQSD. During each run, we employ a 5-fold cross-validation strategy, meticulously training the model on distinct subsets of the data and evaluating its performance across a range of metrics including precision, recall, and F1-score.\n",
        "\n",
        "This approach allows us to assess the model's generalizability and consistency across different data partitions, mitigating the potential for overfitting and providing a comprehensive understanding of its predictive capabilities. The averaged metrics from all five runs are then calculated to present a holistic view of the model's performance, as detailed in our article. The process exemplified in this sample run is critical for ensuring the integrity and validity of our research findings.\n"
      ],
      "metadata": {
        "id": "OcB332ig5vwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa**"
      ],
      "metadata": {
        "id": "zhudKf5n5vwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.regularizers import l1  # Import l1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming merged_df is your DataFrame\n",
        "# merged_df = pd.read_csv('your_dataset.csv') # Uncomment if needed\n",
        "questions = merged_df['Question'].values\n",
        "labels = merged_df['Label_FSQD'].values\n",
        "\n",
        "# Convert labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(labels)\n",
        "labels = np.array(integer_encoded)\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "def tokenize_texts(text_list, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    return tokenizer(text_list, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "def create_roberta_model():\n",
        "    model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=10)\n",
        "    input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    roberta_output = Dropout(0.5)(outputs[0])\n",
        "    classification_output = tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=l1(0.01))(roberta_output)\n",
        "    keras_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=classification_output)\n",
        "    keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    return keras_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "\n",
        "# Lists to hold the macro average precision, recall, and f1-score\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_f1s = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(questions)):\n",
        "    print(f\"Training on fold {fold+1}\")\n",
        "    x_train, x_test = questions[train_index].tolist(), questions[test_index].tolist()\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Tokenize text\n",
        "    x_train_tokenized = tokenize_texts(x_train)\n",
        "    x_test_tokenized = tokenize_texts(x_test)\n",
        "\n",
        "    model = create_roberta_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        {'input_ids': x_train_tokenized['input_ids'], 'attention_mask': x_train_tokenized['attention_mask']},\n",
        "        y_train,\n",
        "        validation_data=(\n",
        "            {'input_ids': x_test_tokenized['input_ids'], 'attention_mask': x_test_tokenized['attention_mask']},\n",
        "            y_test\n",
        "        ),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    #model.save(f\"roberta_fold_{fold+1}.h5\")\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = np.argmax(model.predict(x_test_tokenized.data), axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Append the metrics for macro averaging later\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'history': history.history,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "# Print out the precision, recall, and F1-score for each fold\n",
        "for i, metrics in enumerate(fold_metrics):\n",
        "    print(f\"Fold {i+1} - Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "# Calculate and print the macro average precision, recall, and F1-score across all folds\n",
        "macro_precision = np.mean(all_precisions)\n",
        "macro_recall = np.mean(all_recalls)\n",
        "macro_f1 = np.mean(all_f1s)\n",
        "print(f\"Macro Average Precision: {macro_precision:.4f}\")\n",
        "print(f\"Macro Average Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T08:30:31.078606Z",
          "iopub.execute_input": "2024-02-25T08:30:31.078898Z",
          "iopub.status.idle": "2024-02-25T09:41:35.728873Z",
          "shell.execute_reply.started": "2024-02-25T08:30:31.078869Z",
          "shell.execute_reply": "2024-02-25T09:41:35.727533Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "f15913b794fd4481b5692b9a8078c323",
            "03c1fbfb27744f8dbdedd0195c5595ef",
            "79e98b6e68174f3c8c1908141ba56e5b",
            "c4ad8bc826ad46808b561fb90a533b98",
            "d4e6c5626b5b4d86bd840c8a9e41bd20",
            "b9bcafcc352142dd983df8942cc12a1a"
          ]
        },
        "id": "xka68m-h5vwW",
        "outputId": "fd0036db-9b1a-4245-e826-4ac87c7974af"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-02-25 08:30:40.317442: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 08:30:40.317543: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 08:30:40.466615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f15913b794fd4481b5692b9a8078c323"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03c1fbfb27744f8dbdedd0195c5595ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79e98b6e68174f3c8c1908141ba56e5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4ad8bc826ad46808b561fb90a533b98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4e6c5626b5b4d86bd840c8a9e41bd20"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training on fold 1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9bcafcc352142dd983df8942cc12a1a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708849895.281270     102 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "250/250 [==============================] - 198s 622ms/step - loss: 1.7504 - accuracy: 0.4986 - val_loss: 0.6413 - val_accuracy: 0.9490\nEpoch 2/5\n250/250 [==============================] - 155s 620ms/step - loss: 0.8888 - accuracy: 0.8207 - val_loss: 0.4923 - val_accuracy: 0.9635\nEpoch 3/5\n250/250 [==============================] - 156s 625ms/step - loss: 0.7801 - accuracy: 0.8422 - val_loss: 0.4482 - val_accuracy: 0.9710\nEpoch 4/5\n250/250 [==============================] - 157s 627ms/step - loss: 0.7175 - accuracy: 0.8484 - val_loss: 0.4684 - val_accuracy: 0.9680\nEpoch 5/5\n250/250 [==============================] - 157s 626ms/step - loss: 0.6839 - accuracy: 0.8574 - val_loss: 0.4465 - val_accuracy: 0.9665\n63/63 [==============================] - 16s 196ms/step\nTraining on fold 2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 197s 638ms/step - loss: 1.6472 - accuracy: 0.5459 - val_loss: 0.5321 - val_accuracy: 0.9640\nEpoch 2/5\n250/250 [==============================] - 156s 623ms/step - loss: 0.8500 - accuracy: 0.8403 - val_loss: 0.4441 - val_accuracy: 0.9685\nEpoch 3/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.7220 - accuracy: 0.8630 - val_loss: 0.3898 - val_accuracy: 0.9715\nEpoch 4/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.6852 - accuracy: 0.8622 - val_loss: 0.3734 - val_accuracy: 0.9775\nEpoch 5/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.6292 - accuracy: 0.8808 - val_loss: 0.3773 - val_accuracy: 0.9800\n63/63 [==============================] - 16s 196ms/step\nTraining on fold 3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 200s 650ms/step - loss: 1.7843 - accuracy: 0.4919 - val_loss: 0.5925 - val_accuracy: 0.9665\nEpoch 2/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.9043 - accuracy: 0.8033 - val_loss: 0.4579 - val_accuracy: 0.9780\nEpoch 3/5\n250/250 [==============================] - 157s 627ms/step - loss: 0.7809 - accuracy: 0.8294 - val_loss: 0.4189 - val_accuracy: 0.9775\nEpoch 4/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.7364 - accuracy: 0.8381 - val_loss: 0.4381 - val_accuracy: 0.9770\nEpoch 5/5\n250/250 [==============================] - 157s 627ms/step - loss: 0.7005 - accuracy: 0.8504 - val_loss: 0.4461 - val_accuracy: 0.9760\n63/63 [==============================] - 16s 196ms/step\nTraining on fold 4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 198s 637ms/step - loss: 1.6851 - accuracy: 0.5374 - val_loss: 0.5817 - val_accuracy: 0.9555\nEpoch 2/5\n250/250 [==============================] - 156s 624ms/step - loss: 0.8529 - accuracy: 0.8401 - val_loss: 0.4743 - val_accuracy: 0.9645\nEpoch 3/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.7319 - accuracy: 0.8669 - val_loss: 0.4279 - val_accuracy: 0.9705\nEpoch 4/5\n250/250 [==============================] - 157s 627ms/step - loss: 0.6761 - accuracy: 0.8748 - val_loss: 0.4138 - val_accuracy: 0.9735\nEpoch 5/5\n250/250 [==============================] - 156s 626ms/step - loss: 0.6343 - accuracy: 0.8819 - val_loss: 0.3970 - val_accuracy: 0.9765\n63/63 [==============================] - 16s 196ms/step\nTraining on fold 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 199s 641ms/step - loss: 1.7357 - accuracy: 0.5196 - val_loss: 0.6136 - val_accuracy: 0.9610\nEpoch 2/5\n250/250 [==============================] - 156s 625ms/step - loss: 0.9173 - accuracy: 0.8084 - val_loss: 0.4855 - val_accuracy: 0.9695\nEpoch 3/5\n250/250 [==============================] - 157s 626ms/step - loss: 0.7841 - accuracy: 0.8379 - val_loss: 0.4661 - val_accuracy: 0.9680\nEpoch 4/5\n250/250 [==============================] - 157s 626ms/step - loss: 0.7327 - accuracy: 0.8521 - val_loss: 0.4528 - val_accuracy: 0.9725\nEpoch 5/5\n250/250 [==============================] - 157s 627ms/step - loss: 0.7081 - accuracy: 0.8575 - val_loss: 0.4026 - val_accuracy: 0.9765\n63/63 [==============================] - 16s 195ms/step\nFold 1 - Precision: 0.9669, Recall: 0.9673, F1-Score: 0.9668\nFold 2 - Precision: 0.9800, Recall: 0.9802, F1-Score: 0.9800\nFold 3 - Precision: 0.9761, Recall: 0.9765, F1-Score: 0.9760\nFold 4 - Precision: 0.9758, Recall: 0.9749, F1-Score: 0.9753\nFold 5 - Precision: 0.9768, Recall: 0.9770, F1-Score: 0.9768\nMacro Average Precision: 0.9751\nMacro Average Recall: 0.9752\nMacro Average F1-Score: 0.9750\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "X_eUlFcD5vwX"
      }
    }
  ]
}