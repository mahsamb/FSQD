{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Exploration of FQSC model"
      ],
      "metadata": {
        "id": "Qk6xUn7CAPoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Notebook\n",
        "\n",
        "This notebook commences with the loading of the FQSD. Following the dataset preparation, we delve into the application of the BERT model, a state-of-the-art natural language processing tool, to analyze and extract insights from the FQSD. Our goal is to leverage BERT's advanced capabilities to understand the nuances within the dataset, enabling us to classify and interpret the questions more effectively.\n"
      ],
      "metadata": {
        "id": "eVoQZt4KAPou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Download and Processing**:\n",
        "\n",
        "This code snippet demonstrates the process of downloading a dataset from a remote URL, unzipping it, and loading it into a Pandas DataFrame. The dataset, in this case, is the FSQD-Json-dataset. This initial step is crucial for preparing the data for subsequent analysis and visualization."
      ],
      "metadata": {
        "id": "A0RHiNahAPov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data_url = \"https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip\"\n",
        "zip_filename = \"FSQD-Json-dataset.zip\"\n",
        "\n",
        "# Downloading using requests\n",
        "response = requests.get(data_url)\n",
        "\n",
        "# Check if the request was successful (status_code 200)\n",
        "if response.status_code == 200:\n",
        "    with open(zip_filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the data: {response.status_code}: {response.text}\")\n",
        "    # Add additional error handling here\n",
        "\n",
        "# Unzipping the dataset\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"FSQD-Json-dataset\")\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir(\"FSQD-Json-dataset\"))\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The file doesn’t appear to be a valid zip file\")\n",
        "\n",
        "\n",
        "\n",
        "json_file_path = 'FSQD-Json-dataset/FSQD-Json-dataset.json'  # Update with the correct file path\n",
        "\n",
        "# Try reading the file as a JSON Lines file\n",
        "try:\n",
        "    merged_df = pd.read_json(json_file_path, lines=True)\n",
        "except ValueError as e:\n",
        "    print(f\"Error reading the JSON file: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T14:44:19.204448Z",
          "iopub.execute_input": "2024-02-25T14:44:19.204793Z",
          "iopub.status.idle": "2024-02-25T14:44:21.987423Z",
          "shell.execute_reply.started": "2024-02-25T14:44:19.204767Z",
          "shell.execute_reply": "2024-02-25T14:44:21.986447Z"
        },
        "trusted": true,
        "id": "NJ_wQlzFAPov",
        "outputId": "a5df81e6-f0b9-4900-be28-9e97b85fe56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files extracted:\n['Yu_et_al_2012-Json-dataset.json', 'SubjQA-Json-dataset.json', 'FSQD-Json-dataset.json', 'ConvEx-Json-dataset.json']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T14:44:21.989429Z",
          "iopub.execute_input": "2024-02-25T14:44:21.989795Z",
          "iopub.status.idle": "2024-02-25T14:44:21.996188Z",
          "shell.execute_reply.started": "2024-02-25T14:44:21.989760Z",
          "shell.execute_reply": "2024-02-25T14:44:21.994994Z"
        },
        "trusted": true,
        "id": "eWc8dGWpAPo0",
        "outputId": "579e428e-f94d-4ea6-82da-e9db5ffdf00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Index(['Question', 'Label_FSQD', 'Label_Subjectivity', 'Label_ComparisionForm',\n       'Label_Subjectivity_ComparisionForm', 'Label_SubjectivityType'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads and processes the data from [FSQD-Json-dataset](https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip)."
      ],
      "metadata": {
        "id": "3rStQ1FdAPo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "uYkRvRr_APo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup for Deep Learning\n",
        "\n",
        "Before diving into the model training and evaluation, we need to set up our environment with all the necessary libraries and frameworks:\n"
      ],
      "metadata": {
        "id": "3JNC0Y9mAPo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow sklearn transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T14:44:21.997516Z",
          "iopub.execute_input": "2024-02-25T14:44:21.997874Z",
          "iopub.status.idle": "2024-02-25T14:44:24.873030Z",
          "shell.execute_reply.started": "2024-02-25T14:44:21.997842Z",
          "shell.execute_reply": "2024-02-25T14:44:24.871884Z"
        },
        "trusted": true,
        "id": "p1kEENMVAPo1",
        "outputId": "21df6bd9-d476-4741-8421-bba0e9a31416"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "srrnqNNtAPo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation with BERT\n",
        "\n",
        "This section of the code details training and evaluating a BERT-based model for sequence classification tasks within the FQSD. We initiate by tokenizing the dataset questions using BERT's tokenizer and encoding the labels into integers for processing. Our model, built upon the TFBertForSequenceClassification architecture, includes a dropout layer for regularization and a dense layer tailored to our dataset's number of classes.\n",
        "\n",
        "We leverage K-Fold cross-validation, specifically a 5-fold strategy, to ensure our model's robustness and generalizability. For each fold, we train the model, track its history, and evaluate its performance using precision, recall, and F1-score metrics. This approach not only validates our model's effectiveness but also provides a comprehensive overview of its predictive capabilities.\n",
        "\n",
        "By iterating through each fold, we collect a set of metrics that, once macro-averaged, offer insights into the overall performance of our model across different subsets of the data. The precision, recall, and F1-scores for each fold are reported, followed by the macro-averaged values, culminating in a robust assessment of our classification model's performance.\n"
      ],
      "metadata": {
        "id": "lk_wiJ_yAPo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "KOQK4HVXAPo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Run for Model Evaluation\n",
        "\n",
        "In alignment with our article's methodology, the final results presented are derived from the average of five separate runs to ensure the robustness and reliability of our findings. The code segment showcased here represents just one of these runs, offering a glimpse into the process of training and evaluating our BERT-based model on the FQSD. During each run, we employ a 5-fold cross-validation strategy, meticulously training the model on distinct subsets of the data and evaluating its performance across a range of metrics including precision, recall, and F1-score.\n",
        "\n",
        "This approach allows us to assess the model's generalizability and consistency across different data partitions, mitigating the potential for overfitting and providing a comprehensive understanding of its predictive capabilities. The averaged metrics from all five runs are then calculated to present a holistic view of the model's performance, as detailed in our article. The process exemplified in this sample run is critical for ensuring the integrity and validity of our research findings.\n"
      ],
      "metadata": {
        "id": "emNCt0HAAPo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT**"
      ],
      "metadata": {
        "id": "4LESIC0ZAPo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming merged_df is your DataFrame\n",
        "# merged_df = pd.read_csv('your_dataset.csv') # Uncomment if needed\n",
        "questions = merged_df['Question'].values\n",
        "labels = merged_df['Label_FSQD'].values\n",
        "\n",
        "# Convert labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(labels)\n",
        "labels = np.array(integer_encoded)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "def tokenize_texts(text_list, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    return tokenizer(text_list, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "def create_bert_model():\n",
        "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=10)\n",
        "    input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    bert_output = Dropout(0.5)(outputs[0])\n",
        "    classification_output = tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=l1(0.01))(bert_output)\n",
        "    keras_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=classification_output)\n",
        "    keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    return keras_model\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "\n",
        "# Lists to hold the macro average precision, recall, and f1-score\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_f1s = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(questions)):\n",
        "    print(f\"Training on fold {fold+1}\")\n",
        "    x_train, x_test = questions[train_index].tolist(), questions[test_index].tolist()\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Tokenize text\n",
        "    x_train_tokenized = tokenize_texts(x_train)\n",
        "    x_test_tokenized = tokenize_texts(x_test)\n",
        "\n",
        "    model = create_bert_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        {'input_ids': x_train_tokenized['input_ids'], 'attention_mask': x_train_tokenized['attention_mask']},\n",
        "        y_train,\n",
        "        validation_data=(\n",
        "            {'input_ids': x_test_tokenized['input_ids'], 'attention_mask': x_test_tokenized['attention_mask']},\n",
        "            y_test\n",
        "        ),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save(f\"bert_fold_{fold+1}.h5\")\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = np.argmax(model.predict(x_test_tokenized.data), axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Append the metrics for macro averaging later\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'history': history.history,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "# Print out the precision, recall, and F1-score for each fold\n",
        "for i, metrics in enumerate(fold_metrics):\n",
        "    print(f\"Fold {i+1} - Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "# Calculate and print the macro average precision, recall, and F1-score across all folds\n",
        "macro_precision = np.mean(all_precisions)\n",
        "macro_recall = np.mean(all_recalls)\n",
        "macro_f1 = np.mean(all_f1s)\n",
        "print(f\"Macro Average Precision: {macro_precision:.4f}\")\n",
        "print(f\"Macro Average Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T14:44:24.875352Z",
          "iopub.execute_input": "2024-02-25T14:44:24.875648Z",
          "iopub.status.idle": "2024-02-25T15:58:03.188411Z",
          "shell.execute_reply.started": "2024-02-25T14:44:24.875621Z",
          "shell.execute_reply": "2024-02-25T15:58:03.187265Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "815d7a081e884c57889221d4408e7e4b",
            "f4feb5fe2d204aaf8bd6ce4aa11eee99",
            "cad55bb794e84e669e4b99b3b7bb7a01",
            "0c132139c114418f8dd243eb901fb719",
            "481b79232f614592b3017ca6a4137cb8"
          ]
        },
        "id": "QZVe7eyhAPo3",
        "outputId": "d697a79b-785e-464c-e1e2-3abb4ae42f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-02-25 14:44:32.298710: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 14:44:32.298810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 14:44:32.425665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "815d7a081e884c57889221d4408e7e4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4feb5fe2d204aaf8bd6ce4aa11eee99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cad55bb794e84e669e4b99b3b7bb7a01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c132139c114418f8dd243eb901fb719"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training on fold 1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "481b79232f614592b3017ca6a4137cb8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708872326.482886     108 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "250/250 [==============================] - 201s 643ms/step - loss: 1.9475 - accuracy: 0.4545 - val_loss: 0.9809 - val_accuracy: 0.8565\nEpoch 2/5\n250/250 [==============================] - 161s 646ms/step - loss: 1.0936 - accuracy: 0.7671 - val_loss: 0.5641 - val_accuracy: 0.9605\nEpoch 3/5\n250/250 [==============================] - 161s 645ms/step - loss: 0.8336 - accuracy: 0.8459 - val_loss: 0.4709 - val_accuracy: 0.9655\nEpoch 4/5\n250/250 [==============================] - 161s 645ms/step - loss: 0.7510 - accuracy: 0.8625 - val_loss: 0.4206 - val_accuracy: 0.9750\nEpoch 5/5\n250/250 [==============================] - 161s 646ms/step - loss: 0.6884 - accuracy: 0.8767 - val_loss: 0.4365 - val_accuracy: 0.9680\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 17s 209ms/step\nTraining on fold 2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 202s 661ms/step - loss: 2.0643 - accuracy: 0.3963 - val_loss: 1.1909 - val_accuracy: 0.8525\nEpoch 2/5\n250/250 [==============================] - 162s 647ms/step - loss: 1.1791 - accuracy: 0.7679 - val_loss: 0.6206 - val_accuracy: 0.9495\nEpoch 3/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.8775 - accuracy: 0.8478 - val_loss: 0.4683 - val_accuracy: 0.9660\nEpoch 4/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.7570 - accuracy: 0.8689 - val_loss: 0.4511 - val_accuracy: 0.9640\nEpoch 5/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.6966 - accuracy: 0.8831 - val_loss: 0.4530 - val_accuracy: 0.9630\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 16s 208ms/step\nTraining on fold 3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 202s 662ms/step - loss: 2.0833 - accuracy: 0.3695 - val_loss: 1.2996 - val_accuracy: 0.7900\nEpoch 2/5\n250/250 [==============================] - 162s 649ms/step - loss: 1.2790 - accuracy: 0.6954 - val_loss: 0.7304 - val_accuracy: 0.8855\nEpoch 3/5\n250/250 [==============================] - 162s 648ms/step - loss: 0.9559 - accuracy: 0.7909 - val_loss: 0.5590 - val_accuracy: 0.9710\nEpoch 4/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.8318 - accuracy: 0.8256 - val_loss: 0.4890 - val_accuracy: 0.9730\nEpoch 5/5\n250/250 [==============================] - 162s 648ms/step - loss: 0.7556 - accuracy: 0.8410 - val_loss: 0.4421 - val_accuracy: 0.9755\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 16s 209ms/step\nTraining on fold 4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 202s 661ms/step - loss: 1.9772 - accuracy: 0.3921 - val_loss: 1.1341 - val_accuracy: 0.8820\nEpoch 2/5\n250/250 [==============================] - 162s 648ms/step - loss: 1.1863 - accuracy: 0.7189 - val_loss: 0.6783 - val_accuracy: 0.9535\nEpoch 3/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.9281 - accuracy: 0.7921 - val_loss: 0.5379 - val_accuracy: 0.9655\nEpoch 4/5\n250/250 [==============================] - 162s 648ms/step - loss: 0.8188 - accuracy: 0.8194 - val_loss: 0.5077 - val_accuracy: 0.9665\nEpoch 5/5\n250/250 [==============================] - 162s 648ms/step - loss: 0.7630 - accuracy: 0.8286 - val_loss: 0.4865 - val_accuracy: 0.9650\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 16s 209ms/step\nTraining on fold 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n\nSome weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 200s 661ms/step - loss: 2.0267 - accuracy: 0.3640 - val_loss: 1.1450 - val_accuracy: 0.8590\nEpoch 2/5\n250/250 [==============================] - 162s 647ms/step - loss: 1.1550 - accuracy: 0.7486 - val_loss: 0.5856 - val_accuracy: 0.9595\nEpoch 3/5\n250/250 [==============================] - 161s 646ms/step - loss: 0.8656 - accuracy: 0.8307 - val_loss: 0.4602 - val_accuracy: 0.9710\nEpoch 4/5\n250/250 [==============================] - 162s 646ms/step - loss: 0.7713 - accuracy: 0.8440 - val_loss: 0.4174 - val_accuracy: 0.9755\nEpoch 5/5\n250/250 [==============================] - 162s 647ms/step - loss: 0.6894 - accuracy: 0.8686 - val_loss: 0.4055 - val_accuracy: 0.9730\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n/opt/conda/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n  inputs = self._flatten_to_reference_inputs(inputs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 18s 208ms/step\nFold 1 - Precision: 0.9692, Recall: 0.9686, F1-Score: 0.9682\nFold 2 - Precision: 0.9638, Recall: 0.9639, F1-Score: 0.9634\nFold 3 - Precision: 0.9763, Recall: 0.9753, F1-Score: 0.9756\nFold 4 - Precision: 0.9641, Recall: 0.9631, F1-Score: 0.9635\nFold 5 - Precision: 0.9735, Recall: 0.9731, F1-Score: 0.9732\nMacro Average Precision: 0.9694\nMacro Average Recall: 0.9688\nMacro Average F1-Score: 0.9688\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmcdrBTzAPo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "YLoFi03nAPo4"
      }
    }
  ]
}