{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Exploration of FQSC model"
      ],
      "metadata": {
        "id": "Y29zdKqo6XKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Notebook\n",
        "\n",
        "This notebook commences with the loading of the FQSD. Following the dataset preparation, we delve into the application of the XLnet model, a state-of-the-art natural language processing tool, to analyze and extract insights from the FQSD. Our goal is to leverage XLnet's advanced capabilities to understand the nuances within the dataset, enabling us to classify and interpret the questions more effectively.\n"
      ],
      "metadata": {
        "id": "dRxH4Opw6XKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Download and Processing**:\n",
        "\n",
        "This code snippet demonstrates the process of downloading a dataset from a remote URL, unzipping it, and loading it into a Pandas DataFrame. The dataset, in this case, is the FSQD-Json-dataset. This initial step is crucial for preparing the data for subsequent analysis and visualization."
      ],
      "metadata": {
        "id": "83w8sSMI6XKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data_url = \"https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip\"\n",
        "zip_filename = \"FSQD-Json-dataset.zip\"\n",
        "\n",
        "# Downloading using requests\n",
        "response = requests.get(data_url)\n",
        "\n",
        "# Check if the request was successful (status_code 200)\n",
        "if response.status_code == 200:\n",
        "    with open(zip_filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the data: {response.status_code}: {response.text}\")\n",
        "    # Add additional error handling here\n",
        "\n",
        "# Unzipping the dataset\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"FSQD-Json-dataset\")\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir(\"FSQD-Json-dataset\"))\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The file doesn’t appear to be a valid zip file\")\n",
        "\n",
        "\n",
        "\n",
        "json_file_path = 'FSQD-Json-dataset/FSQD-Json-dataset.json'  # Update with the correct file path\n",
        "\n",
        "# Try reading the file as a JSON Lines file\n",
        "try:\n",
        "    merged_df = pd.read_json(json_file_path, lines=True)\n",
        "except ValueError as e:\n",
        "    print(f\"Error reading the JSON file: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T06:43:29.333850Z",
          "iopub.execute_input": "2024-02-25T06:43:29.334153Z",
          "iopub.status.idle": "2024-02-25T06:43:33.500242Z",
          "shell.execute_reply.started": "2024-02-25T06:43:29.334130Z",
          "shell.execute_reply": "2024-02-25T06:43:33.499279Z"
        },
        "trusted": true,
        "id": "cJyFRLLb6XKh",
        "outputId": "f68f745f-a25d-4b23-a300-69d8ab5b6cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files extracted:\n['Yu_et_al_2012-Json-dataset.json', 'ConvEx-Json-dataset.json', 'SubjQA-Json-dataset.json', 'FSQD-Json-dataset.json']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T06:43:33.502513Z",
          "iopub.execute_input": "2024-02-25T06:43:33.503060Z",
          "iopub.status.idle": "2024-02-25T06:43:33.508125Z",
          "shell.execute_reply.started": "2024-02-25T06:43:33.503024Z",
          "shell.execute_reply": "2024-02-25T06:43:33.507276Z"
        },
        "trusted": true,
        "id": "N7IR4lOv6XKj",
        "outputId": "b15d768c-e953-4cb1-b9ab-d1c0172abc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Index(['Question', 'Label_FSQD', 'Label_Subjectivity', 'Label_ComparisionForm',\n       'Label_Subjectivity_ComparisionForm', 'Label_SubjectivityType'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads and processes the data from [FSQD-Json-dataset](https://github.com/mahsamb/FSQD/raw/main/FSQD-Json-dataset.zip)."
      ],
      "metadata": {
        "id": "kKPuZk8g6XKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "-YVhdvvd6XKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup for Deep Learning\n",
        "\n",
        "Before diving into the model training and evaluation, we need to set up our environment with all the necessary libraries and frameworks:\n"
      ],
      "metadata": {
        "id": "3bIlgRs06XKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow sklearn transformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T06:43:33.512108Z",
          "iopub.execute_input": "2024-02-25T06:43:33.512424Z",
          "iopub.status.idle": "2024-02-25T06:43:36.584463Z",
          "shell.execute_reply.started": "2024-02-25T06:43:33.512394Z",
          "shell.execute_reply": "2024-02-25T06:43:36.583286Z"
        },
        "trusted": true,
        "id": "XggQ_k4k6XKm",
        "outputId": "7b77a876-21d5-4d96-dc54-c8b72c20ded0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25h",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "cuHdgHhs6XKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation with XLnet\n",
        "\n",
        "This section of the code details training and evaluating a XLnet-based model for sequence classification tasks within the FQSD. We initiate by tokenizing the dataset questions using XLnet's tokenizer and encoding the labels into integers for processing. Our model, built upon the TFXLNetForSequenceClassification architecture, includes a dropout layer for regularization and a dense layer tailored to our dataset's number of classes.\n",
        "\n",
        "We leverage K-Fold cross-validation, specifically a 5-fold strategy, to ensure our model's robustness and generalizability. For each fold, we train the model, track its history, and evaluate its performance using precision, recall, and F1-score metrics. This approach not only validates our model's effectiveness but also provides a comprehensive overview of its predictive capabilities.\n",
        "\n",
        "By iterating through each fold, we collect a set of metrics that, once macro-averaged, offer insights into the overall performance of our model across different subsets of the data. The precision, recall, and F1-scores for each fold are reported, followed by the macro-averaged values, culminating in a robust assessment of our classification model's performance.\n"
      ],
      "metadata": {
        "id": "_zN6fPOZ6XKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "Ev56U8Lm6XKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Run for Model Evaluation\n",
        "\n",
        "In alignment with our article's methodology, the final results presented are derived from the average of five separate runs to ensure the robustness and reliability of our findings. The code segment showcased here represents just one of these runs, offering a glimpse into the process of training and evaluating our XLnet-based model on the FQSD. During each run, we employ a 5-fold cross-validation strategy, meticulously training the model on distinct subsets of the data and evaluating its performance across a range of metrics including precision, recall, and F1-score.\n",
        "\n",
        "This approach allows us to assess the model's generalizability and consistency across different data partitions, mitigating the potential for overfitting and providing a comprehensive understanding of its predictive capabilities. The averaged metrics from all five runs are then calculated to present a holistic view of the model's performance, as detailed in our article. The process exemplified in this sample run is critical for ensuring the integrity and validity of our research findings.\n"
      ],
      "metadata": {
        "id": "1Mgntl5q6XKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XLnet**"
      ],
      "metadata": {
        "id": "Mow3tOdk6XKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming merged_df is your DataFrame\n",
        "# merged_df = pd.read_csv('your_dataset.csv') # Uncomment if needed\n",
        "questions = merged_df['Question'].values\n",
        "labels = merged_df['Label_FSQD'].values\n",
        "\n",
        "# Convert labels to integers\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(labels)\n",
        "labels = np.array(integer_encoded)\n",
        "\n",
        "# Load XLNet tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "def tokenize_texts(text_list, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    return tokenizer(text_list, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "\n",
        "def create_xlnet_model():\n",
        "    model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=10)\n",
        "    input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    xlnet_output = Dropout(0.5)(outputs[0])\n",
        "    classification_output = tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=l2(0.01))(xlnet_output)\n",
        "    keras_model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=classification_output)\n",
        "    keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                        loss='sparse_categorical_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    return keras_model\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_metrics = []\n",
        "\n",
        "# Lists to hold the macro average precision, recall, and f1-score\n",
        "all_precisions = []\n",
        "all_recalls = []\n",
        "all_f1s = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(questions)):\n",
        "    print(f\"Training on fold {fold+1}\")\n",
        "    x_train, x_test = questions[train_index].tolist(), questions[test_index].tolist()\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Tokenize text\n",
        "    x_train_tokenized = tokenize_texts(x_train)\n",
        "    x_test_tokenized = tokenize_texts(x_test)\n",
        "\n",
        "    model = create_xlnet_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        {'input_ids': x_train_tokenized['input_ids'], 'attention_mask': x_train_tokenized['attention_mask']},\n",
        "        y_train,\n",
        "        validation_data=(\n",
        "            {'input_ids': x_test_tokenized['input_ids'], 'attention_mask': x_test_tokenized['attention_mask']},\n",
        "            y_test\n",
        "        ),\n",
        "        epochs=5,\n",
        "        batch_size=32,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    model.save(f\"xlnet_fold_{fold+1}.h5\")\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = np.argmax(model.predict({'input_ids': x_test_tokenized['input_ids'], 'attention_mask': x_test_tokenized['attention_mask']}), axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "    # Append the metrics for macro averaging later\n",
        "    all_precisions.append(precision)\n",
        "    all_recalls.append(recall)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'history': history.history,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "# Print out the precision, recall, and F1-score for each fold\n",
        "for i, metrics in enumerate(fold_metrics):\n",
        "    print(f\"Fold {i+1} - Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1']:.4f}\")\n",
        "\n",
        "# Calculate and print the macro average precision, recall, and F1-score across all folds\n",
        "macro_precision = np.mean(all_precisions)\n",
        "macro_recall = np.mean(all_recalls)\n",
        "macro_f1 = np.mean(all_f1s)\n",
        "print(f\"Macro Average Precision: {macro_precision:.4f}\")\n",
        "print(f\"MacroAverage Recall: {macro_recall:.4f}\")\n",
        "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-25T06:43:36.587165Z",
          "iopub.execute_input": "2024-02-25T06:43:36.587480Z",
          "iopub.status.idle": "2024-02-25T08:18:48.031714Z",
          "shell.execute_reply.started": "2024-02-25T06:43:36.587449Z",
          "shell.execute_reply": "2024-02-25T08:18:48.030812Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "13d20d60581b45198458873ed7656104",
            "7d864282c0ec4a05bda03bc6a0476759",
            "8ce05dcb33bc4adf9eb37b98ba1a9ff1",
            "62053627b4844671900a3c3fee5d670f"
          ]
        },
        "id": "MgFGfl_d6XKn",
        "outputId": "49fb9010-769a-410d-d61b-4cb6e4fa3ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-02-25 06:43:50.695084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 06:43:50.695176: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 06:43:50.970922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13d20d60581b45198458873ed7656104"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d864282c0ec4a05bda03bc6a0476759"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ce05dcb33bc4adf9eb37b98ba1a9ff1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Training on fold 1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tf_model.h5:   0%|          | 0.00/565M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62053627b4844671900a3c3fee5d670f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1708843484.483882      91 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "250/250 [==============================] - 252s 855ms/step - loss: 1.5693 - accuracy: 0.4827 - val_loss: 0.6507 - val_accuracy: 0.8735\nEpoch 2/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.8604 - accuracy: 0.7448 - val_loss: 0.3537 - val_accuracy: 0.9530\nEpoch 3/5\n250/250 [==============================] - 214s 854ms/step - loss: 0.6738 - accuracy: 0.8043 - val_loss: 0.3147 - val_accuracy: 0.9600\nEpoch 4/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.6056 - accuracy: 0.8191 - val_loss: 0.3104 - val_accuracy: 0.9555\nEpoch 5/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.5609 - accuracy: 0.8280 - val_loss: 0.2840 - val_accuracy: 0.9630\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 19s 260ms/step\nTraining on fold 2\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 252s 877ms/step - loss: 1.5514 - accuracy: 0.5025 - val_loss: 0.5394 - val_accuracy: 0.8625\nEpoch 2/5\n250/250 [==============================] - 214s 857ms/step - loss: 0.8201 - accuracy: 0.7666 - val_loss: 0.3143 - val_accuracy: 0.9540\nEpoch 3/5\n250/250 [==============================] - 218s 872ms/step - loss: 0.6441 - accuracy: 0.8238 - val_loss: 0.2675 - val_accuracy: 0.9590\nEpoch 4/5\n250/250 [==============================] - 214s 856ms/step - loss: 0.5718 - accuracy: 0.8521 - val_loss: 0.2490 - val_accuracy: 0.9660\nEpoch 5/5\n250/250 [==============================] - 214s 856ms/step - loss: 0.5257 - accuracy: 0.8627 - val_loss: 0.2491 - val_accuracy: 0.9650\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 19s 261ms/step\nTraining on fold 3\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 250s 869ms/step - loss: 1.6504 - accuracy: 0.4535 - val_loss: 0.4830 - val_accuracy: 0.9330\nEpoch 2/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.8172 - accuracy: 0.7610 - val_loss: 0.2737 - val_accuracy: 0.9695\nEpoch 3/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.6325 - accuracy: 0.8180 - val_loss: 0.2454 - val_accuracy: 0.9750\nEpoch 4/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.5774 - accuracy: 0.8346 - val_loss: 0.2812 - val_accuracy: 0.9670\nEpoch 5/5\n250/250 [==============================] - 214s 856ms/step - loss: 0.5297 - accuracy: 0.8464 - val_loss: 0.2410 - val_accuracy: 0.9745\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 19s 261ms/step\nTraining on fold 4\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 252s 867ms/step - loss: 1.6654 - accuracy: 0.4451 - val_loss: 0.5481 - val_accuracy: 0.8830\nEpoch 2/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.8341 - accuracy: 0.7514 - val_loss: 0.2552 - val_accuracy: 0.9645\nEpoch 3/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.6414 - accuracy: 0.8117 - val_loss: 0.2467 - val_accuracy: 0.9620\nEpoch 4/5\n250/250 [==============================] - 214s 856ms/step - loss: 0.5892 - accuracy: 0.8295 - val_loss: 0.2156 - val_accuracy: 0.9700\nEpoch 5/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.5467 - accuracy: 0.8346 - val_loss: 0.2368 - val_accuracy: 0.9680\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 19s 261ms/step\nTraining on fold 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n  warnings.warn(\nSome layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetForSequenceClassification: ['lm_loss']\n- This IS expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFXLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary', 'logits_proj']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/5\n250/250 [==============================] - 253s 868ms/step - loss: 1.7742 - accuracy: 0.4030 - val_loss: 0.6791 - val_accuracy: 0.8285\nEpoch 2/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.8358 - accuracy: 0.7657 - val_loss: 0.3139 - val_accuracy: 0.9540\nEpoch 3/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.6716 - accuracy: 0.8177 - val_loss: 0.2684 - val_accuracy: 0.9635\nEpoch 4/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.5623 - accuracy: 0.8445 - val_loss: 0.2201 - val_accuracy: 0.9730\nEpoch 5/5\n250/250 [==============================] - 214s 855ms/step - loss: 0.5091 - accuracy: 0.8591 - val_loss: 0.2390 - val_accuracy: 0.9760\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "63/63 [==============================] - 19s 260ms/step\nFold 1 - Precision: 0.9637, Recall: 0.9638, F1-Score: 0.9630\nFold 2 - Precision: 0.9669, Recall: 0.9649, F1-Score: 0.9654\nFold 3 - Precision: 0.9745, Recall: 0.9748, F1-Score: 0.9744\nFold 4 - Precision: 0.9674, Recall: 0.9662, F1-Score: 0.9663\nFold 5 - Precision: 0.9765, Recall: 0.9760, F1-Score: 0.9761\nMacro Average Precision: 0.9698\nMacroAverage Recall: 0.9691\nMacro Average F1-Score: 0.9691\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKsChkVo6XKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#######################################################################################################"
      ],
      "metadata": {
        "id": "f0DUMm_n6XKp"
      }
    }
  ]
}